{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11594255,"sourceType":"datasetVersion","datasetId":7270567},{"sourceId":11594671,"sourceType":"datasetVersion","datasetId":7270850},{"sourceId":11594758,"sourceType":"datasetVersion","datasetId":7270922},{"sourceId":11603898,"sourceType":"datasetVersion","datasetId":7277945}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torchaudio.models\nimport tarfile\n\n#import nemo.collections.asr as nemo_asr\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:32:03.891410Z","iopub.execute_input":"2025-04-28T18:32:03.891755Z","iopub.status.idle":"2025-04-28T18:32:10.812894Z","shell.execute_reply.started":"2025-04-28T18:32:03.891727Z","shell.execute_reply":"2025-04-28T18:32:10.811899Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"markdown","source":"## Import Data","metadata":{}},{"cell_type":"markdown","source":"Female Control","metadata":{}},{"cell_type":"code","source":"# Root directory\nroot_dir = '/kaggle/input/torgo-f-con'\n\n# Prepare an empty list to store the data\ndata = []\n\n# Loop through each speaker folder\nfor speaker_id in os.listdir(root_dir):\n    speaker_path = os.path.join(root_dir, speaker_id)\n    if os.path.isdir(speaker_path):\n        session_path = os.path.join(speaker_path, 'Session1')\n        \n        prompts_path = os.path.join(session_path, 'prompts')\n        audio_path = os.path.join(session_path, 'wav_arrayMic')\n        \n        if os.path.exists(prompts_path) and os.path.exists(audio_path):\n            # For each transcription file\n            for txt_file in os.listdir(prompts_path):\n                if txt_file.endswith('.txt'):\n                    base_filename = os.path.splitext(txt_file)[0]\n                    \n                    txt_full_path = os.path.join(prompts_path, txt_file)\n                    wav_full_path = os.path.join(audio_path, f\"{base_filename}.wav\")\n                    \n                    if os.path.exists(wav_full_path):\n                        # Read the actual text\n                        with open(txt_full_path, 'r', encoding='utf-8') as f:\n                            transcription_text = f.read().strip()  # Remove leading/trailing spaces\n                        \n                        # CLEANING \n                        transcription_text = transcription_text.lower().strip()  # normalize to lower-case\n                        \n                        # Delete if it references a jpg\n                        if '.jpg' in transcription_text:\n                            continue\n                        # Delete if 'xxx'\n                        if transcription_text == 'xxx':\n                            continue\n                        # Delete if empty\n                        if transcription_text == '':\n                            continue\n                        # Delete if 'say'\n                        if transcription_text == 'say':\n                            continue\n                        \n                        # If it contains multiple words, keep only the first word\n                        first_word = transcription_text.split()[0]\n                        \n                        # Add to the dataset\n                        data.append({\n                            'Audio_path': wav_full_path,\n                            'Transcription': first_word,\n                            'Sex': 'F',\n                            'Group': 'Control'\n                        })\n\n# Turn it into a DataFrame\nfCon_df = pd.DataFrame(data)\n\nprint(fCon_df.head())\nprint(f\"Total examples collected: {len(fCon_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:53:21.915586Z","iopub.execute_input":"2025-04-28T18:53:21.915947Z","iopub.status.idle":"2025-04-28T18:53:24.183254Z","shell.execute_reply.started":"2025-04-28T18:53:21.915922Z","shell.execute_reply":"2025-04-28T18:53:24.182267Z"}},"outputs":[{"name":"stdout","text":"                                          Audio_path Transcription Sex  \\\n0  /kaggle/input/torgo-f-con/FC03/Session1/wav_ar...          cart   F   \n1  /kaggle/input/torgo-f-con/FC03/Session1/wav_ar...          dark   F   \n2  /kaggle/input/torgo-f-con/FC03/Session1/wav_ar...           and   F   \n3  /kaggle/input/torgo-f-con/FC03/Session1/wav_ar...       trouble   F   \n4  /kaggle/input/torgo-f-con/FC03/Session1/wav_ar...         start   F   \n\n     Group  \n0  Control  \n1  Control  \n2  Control  \n3  Control  \n4  Control  \nTotal examples collected: 552\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"Female Dysarthria","metadata":{}},{"cell_type":"code","source":"# Root directory\nroot_dir = '/kaggle/input/torgo-f-dys'\n\n# Prepare an empty list to store the data\ndata = []\n\n# Loop through each speaker folder\nfor speaker_id in os.listdir(root_dir):\n    speaker_path = os.path.join(root_dir, speaker_id)\n    if os.path.isdir(speaker_path):\n        session_path = os.path.join(speaker_path, 'Session1')\n        \n        prompts_path = os.path.join(session_path, 'prompts')\n        audio_path = os.path.join(session_path, 'wav_arrayMic')\n        \n        if os.path.exists(prompts_path) and os.path.exists(audio_path):\n            # For each transcription file\n            for txt_file in os.listdir(prompts_path):\n                if txt_file.endswith('.txt'):\n                    base_filename = os.path.splitext(txt_file)[0]\n                    \n                    txt_full_path = os.path.join(prompts_path, txt_file)\n                    wav_full_path = os.path.join(audio_path, f\"{base_filename}.wav\")\n                    \n                    if os.path.exists(wav_full_path):\n                        # Read the actual text\n                        with open(txt_full_path, 'r', encoding='utf-8') as f:\n                            transcription_text = f.read().strip()  # Remove leading/trailing spaces\n                        \n                        # CLEANING \n                        transcription_text = transcription_text.lower().strip()  # normalize to lower-case\n                        \n                        # Delete if it references a jpg\n                        if '.jpg' in transcription_text:\n                            continue\n                        # Delete if 'xxx'\n                        if transcription_text == 'xxx':\n                            continue\n                        # Delete if empty\n                        if transcription_text == '':\n                            continue\n                        # Delete if 'say'\n                        if transcription_text == 'say':\n                            continue\n                        \n                        # If it contains multiple words, keep only the first word\n                        first_word = transcription_text.split()[0]\n                        \n                        # Add to the dataset\n                        data.append({\n                            'Audio_path': wav_full_path,\n                            'Transcription': first_word,\n                            'Sex': 'F',\n                            'Group': 'Dysarthria'\n                        })\n\n# Turn it into a DataFrame\nfDys_df = pd.DataFrame(data)\n\nprint(fDys_df.head())\nprint(f\"Total examples collected: {len(fDys_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:53:16.767990Z","iopub.execute_input":"2025-04-28T18:53:16.768387Z","iopub.status.idle":"2025-04-28T18:53:19.030231Z","shell.execute_reply.started":"2025-04-28T18:53:16.768362Z","shell.execute_reply":"2025-04-28T18:53:19.029237Z"}},"outputs":[{"name":"stdout","text":"                                          Audio_path Transcription Sex  \\\n0  /kaggle/input/torgo-f-dys/F04/Session1/wav_arr...          rate   F   \n1  /kaggle/input/torgo-f-dys/F04/Session1/wav_arr...          glow   F   \n2  /kaggle/input/torgo-f-dys/F04/Session1/wav_arr...            go   F   \n3  /kaggle/input/torgo-f-dys/F04/Session1/wav_arr...          raid   F   \n4  /kaggle/input/torgo-f-dys/F04/Session1/wav_arr...         trace   F   \n\n        Group  \n0  Dysarthria  \n1  Dysarthria  \n2  Dysarthria  \n3  Dysarthria  \n4  Dysarthria  \nTotal examples collected: 527\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"Male Control","metadata":{}},{"cell_type":"code","source":"# Root directory\nroot_dir = '/kaggle/input/torgo-mcons/MC(1)'\n\n# Prepare an empty list to store the data\ndata = []\n\n# Loop through each speaker folder\nfor speaker_id in os.listdir(root_dir):\n    speaker_path = os.path.join(root_dir, speaker_id)\n    if os.path.isdir(speaker_path):\n        session_path = os.path.join(speaker_path, 'Session1')\n        \n        prompts_path = os.path.join(session_path, 'prompts')\n        audio_path = os.path.join(session_path, 'wav_arrayMic')\n        \n        if os.path.exists(prompts_path) and os.path.exists(audio_path):\n            # For each transcription file\n            for txt_file in os.listdir(prompts_path):\n                if txt_file.endswith('.txt'):\n                    base_filename = os.path.splitext(txt_file)[0]\n                    \n                    txt_full_path = os.path.join(prompts_path, txt_file)\n                    wav_full_path = os.path.join(audio_path, f\"{base_filename}.wav\")\n                    \n                    if os.path.exists(wav_full_path):\n                        # Read the actual text\n                        with open(txt_full_path, 'r', encoding='utf-8') as f:\n                            transcription_text = f.read().strip()  # Remove leading/trailing spaces\n                        \n                        # CLEANING \n                        transcription_text = transcription_text.lower().strip()  # normalize to lower-case\n                        \n                        # Delete if it references a jpg\n                        if '.jpg' in transcription_text:\n                            continue\n                        # Delete if 'xxx'\n                        if transcription_text == 'xxx':\n                            continue\n                        # Delete if empty\n                        if transcription_text == '':\n                            continue\n                        # Delete if 'say'\n                        if transcription_text == 'say':\n                            continue\n                        \n                        # If it contains multiple words, keep only the first word\n                        first_word = transcription_text.split()[0]\n                        \n                        # Add to the dataset\n                        data.append({\n                            'Audio_path': wav_full_path,\n                            'Transcription': first_word,\n                            'Sex': 'M',\n                            'Group': 'Control'\n                        })\n\n# Turn it into a DataFrame\nmCon_df = pd.DataFrame(data)\n\nprint(mCon_df.head())\nprint(f\"Total examples collected: {len(mCon_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:53:10.572115Z","iopub.execute_input":"2025-04-28T18:53:10.572485Z","iopub.status.idle":"2025-04-28T18:53:12.503695Z","shell.execute_reply.started":"2025-04-28T18:53:10.572461Z","shell.execute_reply":"2025-04-28T18:53:12.502642Z"}},"outputs":[{"name":"stdout","text":"                                          Audio_path Transcription Sex  \\\n0  /kaggle/input/torgo-mcons/MC(1)/MC02/Session1/...         train   M   \n1  /kaggle/input/torgo-mcons/MC(1)/MC02/Session1/...          carl   M   \n2  /kaggle/input/torgo-mcons/MC(1)/MC02/Session1/...          raid   M   \n3  /kaggle/input/torgo-mcons/MC(1)/MC02/Session1/...         swing   M   \n4  /kaggle/input/torgo-mcons/MC(1)/MC02/Session1/...        double   M   \n\n     Group  \n0  Control  \n1  Control  \n2  Control  \n3  Control  \n4  Control  \nTotal examples collected: 1940\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Male Dysarthria","metadata":{}},{"cell_type":"code","source":"# Root directory\nroot_dir = '/kaggle/input/torgo-mcons/MC(1)'\n\n# Prepare an empty list to store the data\ndata = []\n\n# Loop through each speaker folder\nfor speaker_id in os.listdir(root_dir):\n    speaker_path = os.path.join(root_dir, speaker_id)\n    if os.path.isdir(speaker_path):\n        session_path = os.path.join(speaker_path, 'Session1')\n        \n        prompts_path = os.path.join(session_path, 'prompts')\n        audio_path = os.path.join(session_path, 'wav_arrayMic')\n        \n        if os.path.exists(prompts_path) and os.path.exists(audio_path):\n            # For each transcription file\n            for txt_file in os.listdir(prompts_path):\n                if txt_file.endswith('.txt'):\n                    base_filename = os.path.splitext(txt_file)[0]\n                    \n                    txt_full_path = os.path.join(prompts_path, txt_file)\n                    wav_full_path = os.path.join(audio_path, f\"{base_filename}.wav\")\n                    \n                    if os.path.exists(wav_full_path):\n                        # Read the actual text\n                        with open(txt_full_path, 'r', encoding='utf-8') as f:\n                            transcription_text = f.read().strip()  # Remove leading/trailing spaces\n                        \n                        # CLEANING \n                        transcription_text = transcription_text.lower().strip()  # normalize to lower-case\n                        \n                        # Delete if it references a jpg\n                        if '.jpg' in transcription_text:\n                            continue\n                        # Delete if 'xxx'\n                        if transcription_text == 'xxx':\n                            continue\n                        # Delete if empty\n                        if transcription_text == '':\n                            continue\n                        # Delete if 'say'\n                        if transcription_text == 'say':\n                            continue\n                        \n                        # If it contains multiple words, keep only the first word\n                        first_word = transcription_text.split()[0]\n                        \n                        # Add to the dataset\n                        data.append({\n                            'Audio_path': wav_full_path,\n                            'Transcription': first_word,\n                            'Sex': 'M',\n                            'Group': 'Dysarthria'\n                        })\n\n# Turn it into a DataFrame\nmDys_df = pd.DataFrame(data)\n\nprint(mDys_df.head())\nprint(f\"Total examples collected: {len(mDys_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:52:55.988753Z","iopub.execute_input":"2025-04-28T18:52:55.989662Z","iopub.status.idle":"2025-04-28T18:53:03.597633Z","shell.execute_reply.started":"2025-04-28T18:52:55.989633Z","shell.execute_reply":"2025-04-28T18:53:03.596510Z"}},"outputs":[{"name":"stdout","text":"                                          Audio_path Transcription Sex  \\\n0  /kaggle/input/torgo-mcons/MC(1)/MC02/Session1/...         train   M   \n1  /kaggle/input/torgo-mcons/MC(1)/MC02/Session1/...          carl   M   \n2  /kaggle/input/torgo-mcons/MC(1)/MC02/Session1/...          raid   M   \n3  /kaggle/input/torgo-mcons/MC(1)/MC02/Session1/...         swing   M   \n4  /kaggle/input/torgo-mcons/MC(1)/MC02/Session1/...        double   M   \n\n        Group  \n0  Dysarthria  \n1  Dysarthria  \n2  Dysarthria  \n3  Dysarthria  \n4  Dysarthria  \nTotal examples collected: 1940\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Merging Datasets","metadata":{}},{"cell_type":"code","source":"# Make Female Con/Dys same length \nfCon_df = fCon_df.sample(n=len(fDys_df), random_state=42).reset_index(drop=True)\n\nprint(f\"Female Control: {len(fCon_df)}\")\nprint(f\"Female Dysarthria: {len(fDys_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:52:49.642526Z","iopub.execute_input":"2025-04-28T18:52:49.642981Z","iopub.status.idle":"2025-04-28T18:52:49.651674Z","shell.execute_reply.started":"2025-04-28T18:52:49.642954Z","shell.execute_reply":"2025-04-28T18:52:49.650415Z"}},"outputs":[{"name":"stdout","text":"Female Control: 527\nFemale Dysarthria: 527\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Merge all datasets\nfull_dataset = pd.concat([fCon_df, fDys_df, mCon_df, mDys_df], ignore_index=True)\n\nprint(full_dataset.head())\nprint(f\"Total Size: {len(full_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T18:53:41.855145Z","iopub.execute_input":"2025-04-28T18:53:41.855509Z","iopub.status.idle":"2025-04-28T18:53:41.864983Z","shell.execute_reply.started":"2025-04-28T18:53:41.855483Z","shell.execute_reply":"2025-04-28T18:53:41.864002Z"}},"outputs":[{"name":"stdout","text":"                                          Audio_path Transcription Sex  \\\n0  /kaggle/input/torgo-f-con/FC03/Session1/wav_ar...          cart   F   \n1  /kaggle/input/torgo-f-con/FC03/Session1/wav_ar...          dark   F   \n2  /kaggle/input/torgo-f-con/FC03/Session1/wav_ar...           and   F   \n3  /kaggle/input/torgo-f-con/FC03/Session1/wav_ar...       trouble   F   \n4  /kaggle/input/torgo-f-con/FC03/Session1/wav_ar...         start   F   \n\n     Group  \n0  Control  \n1  Control  \n2  Control  \n3  Control  \n4  Control  \nTotal Size: 4959\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"### Vanilla RNN-T Class from from Torchaudio\n\nLink: https://pytorch.org/audio/stable/generated/torchaudio.models.RNNT.html#torchaudio.models.RNNT","metadata":{}},{"cell_type":"code","source":"class RNNTDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, transforms=None):\n        self.dataframe = dataframe\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        audio_path = row['audio_path']\n        transcription = row['transcription']\n        \n        # Load the waveform and sample rate\n        waveform, sample_rate = torchaudio.load(audio_path)\n        \n        # Apply transforms (like resampling, normalization, etc.), if any\n        if self.transforms:\n            waveform = self.transforms(waveform)\n        \n        return {\n            'waveform': waveform,\n            'transcription': transcription,\n            'sex': row['sex'],\n            'group': row['group']\n        }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VanillaRNNT(nn.Module):\n    def __init__(self, input_dim, vocab_size, encoder_dim=512, prediction_dim=512, joiner_dim=512):\n        super().__init__()\n        self.rnnt = torchaudio.models.RNNT(\n            input_dim=input_dim,\n            vocab_size=vocab_size,\n            encoder_dim=encoder_dim,\n            prediction_dim=prediction_dim,\n            joiner_dim=joiner_dim,\n        )\n    \n    def forward(self, x, y):\n        return self.rnnt(x, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T22:27:48.771157Z","iopub.execute_input":"2025-04-27T22:27:48.771761Z","iopub.status.idle":"2025-04-27T22:27:48.780023Z","shell.execute_reply.started":"2025-04-27T22:27:48.771729Z","shell.execute_reply":"2025-04-27T22:27:48.778690Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### RNNT with MultiSoftmax + Attention from Paper \n\n\"A. Patil, V. Joshi, P. Agrawal, and R. Mehta, “Streaming bilingual end-to-end asr model using attention over multiple softmax,” Jan. 2023.\n\nLink: https://arxiv.org/pdf/2401.11645","metadata":{}},{"cell_type":"code","source":"# Language Specific Attention Block \nclass LanguageAttentionBlock(nn.Module):\n    def __init__(self, encoder_dim):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(embed_dim=encoder_dim, num_heads=1, batch_first=True)\n        self.fc = nn.Linear(encoder_dim, 2)  # Output: 2 scores (healthy, dysarthria)\n\n    def forward(self, enc_out):\n        attn_output, _ = self.attention(enc_out, enc_out, enc_out)\n        scores = self.fc(attn_output[:, -1, :])  # Use last timestep\n        weights = torch.softmax(scores, dim=-1)\n        return weights[:, 0].unsqueeze(-1), weights[:, 1].unsqueeze(-1)\n\n# Bilingual MultiSoftmax with Attention Block\nclass RNNTMultiSoftmaxAttn(nn.Module):\n    def __init__(self, input_dim, vocab_size, encoder_dim=512, prediction_dim=512, joiner_dim=512):\n        super().__init__()\n        self.encoder = torchaudio.models.EmformerEncoder(\n            input_dim=input_dim, \n            output_dim=encoder_dim,\n            num_heads=8,\n            ff_mult=4,\n            num_layers=4,\n            segment_length=32,\n            right_context_length=8,\n            left_context_length=64,\n        )\n        self.decoder = torchaudio.models.RNNTDecoder(vocab_size, prediction_dim)\n        self.joiner_healthy = torchaudio.models.RNNTJoiner(encoder_dim, prediction_dim, joiner_dim, vocab_size)\n        self.joiner_dysarthria = torchaudio.models.RNNTJoiner(encoder_dim, prediction_dim, joiner_dim, vocab_size)\n        self.language_attention = LanguageAttentionBlock(encoder_dim)\n\n    def forward(self, x, y):\n        enc_out = self.encoder(x)\n        dec_out = self.decoder(y)\n\n        logits_healthy = self.joiner_healthy(enc_out, dec_out)\n        logits_dysarthria = self.joiner_dysarthria(enc_out, dec_out)\n\n        weight_healthy, weight_dysarthria = self.language_attention(enc_out)\n\n        # Combine\n        logits = weight_healthy * logits_healthy + weight_dysarthria * logits_dysarthria\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T22:28:53.719089Z","iopub.execute_input":"2025-04-27T22:28:53.719539Z","iopub.status.idle":"2025-04-27T22:28:53.731021Z","shell.execute_reply.started":"2025-04-27T22:28:53.719510Z","shell.execute_reply":"2025-04-27T22:28:53.729992Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"Running Code Example","metadata":{}},{"cell_type":"code","source":"batch_size, sequence_length, dim = 3, 12345, 80\n\ncuda = torch.cuda.is_available()  \ndevice = torch.device('cuda' if cuda else 'cpu')\n\ninputs = torch.rand(batch_size, sequence_length, dim).to(device)\ninput_lengths = torch.IntTensor([12345, 12300, 12000])\ntargets = torch.LongTensor([[1, 3, 3, 3, 3, 3, 4, 5, 6, 2],\n                            [1, 3, 3, 3, 3, 3, 4, 5, 2, 0],\n                            [1, 3, 3, 3, 3, 3, 4, 2, 0, 0]]).to(device)\ntarget_lengths = torch.LongTensor([9, 8, 7])\n\nmodel = nn.DataParallel(RNNTransducer(num_classes=10)).to(device)\n\n# Forward propagate\noutputs = model(inputs, input_lengths, targets, target_lengths)\n\n# Recognize input speech\noutputs = model.module.recognize(inputs, input_lengths)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}