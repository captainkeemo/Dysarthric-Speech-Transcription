{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMiBT28hWZqdrJASjH/X06x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/captainkeemo/Dysarthric-Speech-Transcription/blob/main/models/base_CTC_RNN_T_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer editdistance --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt16kcAt1rQ5",
        "outputId": "c8873e5d-9d66-4926-bd95-39635f0f1754"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hpptgrn4DkLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7823b6-b859-4973-8c04-621ef809e15e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from jiwer import wer, cer\n",
        "import editdistance\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "CyEKx_H6q-hU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary\n",
        "vocab = list(\"abcdefghijklmnopqrstuvwxyz '\") + [\"|\"]\n",
        "char_to_index = {c: i for i, c in enumerate(vocab)}\n",
        "index_to_char = {i: c for c, i in char_to_index.items()}\n"
      ],
      "metadata": {
        "id": "ociJ8CjFruC-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class that loads audio and transcripts from TORGO\n",
        "class TorgoDataset(Dataset):\n",
        "    def __init__(self, root_dir, char_to_index, sample_rate=16000, items_file=None):\n",
        "        self.char_to_index = char_to_index\n",
        "        self.sample_rate = sample_rate\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=80)\n",
        "\n",
        "        if items_file is not None:\n",
        "            # Load from pre-saved file\n",
        "            self.items = torch.load(items_file)\n",
        "        else:\n",
        "            # Build dataset normally\n",
        "            self.items = []\n",
        "            wav_files = glob.glob(os.path.join(root_dir, '**', 'wav_headMic', '*.wav'), recursive=True)\n",
        "            for wav_fp in tqdm(wav_files, desc=f\"Building Dataset ({len(wav_files)} files)\"):\n",
        "                txt_fp = wav_fp.replace('wav_headMic', 'prompts').replace('.wav', '.txt')\n",
        "                if os.path.exists(txt_fp):\n",
        "                    with open(txt_fp, 'r') as f:\n",
        "                        text = f.read().strip().lower()\n",
        "                    self.items.append((wav_fp, text))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        wav_path, transcript = self.items[idx]\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(wav_path)\n",
        "            if sr != self.sample_rate:\n",
        "                waveform = torchaudio.transforms.Resample(sr, self.sample_rate)(waveform)\n",
        "            spec = self.mel_transform(waveform).squeeze(0).transpose(0, 1)\n",
        "            target = torch.tensor([self.char_to_index[c] for c in transcript if c in self.char_to_index])\n",
        "            return spec, target\n",
        "        except Exception:\n",
        "            return None\n",
        "\n"
      ],
      "metadata": {
        "id": "NF-gwWrBq6zj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch collation for DataLoader: pad inputs and concatenate targets\n",
        "def collate_fn(batch):\n",
        "    # Filter out None samples\n",
        "    batch = [sample for sample in batch if sample is not None]\n",
        "    if len(batch) == 0:\n",
        "        return None\n",
        "    specs, targets = zip(*batch)\n",
        "    spec_lens = torch.tensor([s.size(0) for s in specs], dtype=torch.long)\n",
        "    target_lens = torch.tensor([t.size(0) for t in targets], dtype=torch.long)\n",
        "    specs_padded = pad_sequence(specs, batch_first=True)\n",
        "    targets_flat = torch.cat(targets)\n",
        "    return specs_padded, targets_flat, spec_lens, target_lens\n"
      ],
      "metadata": {
        "id": "IVPwDf-um_OJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the full RNN-T model with encoder, decoder, and joiner\n",
        "class RNNTModel(nn.Module):\n",
        "    def __init__(self, input_dim=80, vocab_size=len(vocab), hidden_dim=128, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.decoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.joiner = nn.Linear(hidden_dim * 3, vocab_size)\n",
        "\n",
        "    def forward(self, encoder_input, target_input):\n",
        "        enc_out, _ = self.encoder(encoder_input)\n",
        "        dec_emb = self.embed(target_input)\n",
        "        dec_out, _ = self.decoder(dec_emb)\n",
        "        enc_exp = enc_out.unsqueeze(2)\n",
        "        dec_exp = dec_out.unsqueeze(1)\n",
        "        join = torch.cat([\n",
        "            enc_exp.expand(-1, -1, dec_exp.size(2), -1),\n",
        "            dec_exp.expand(-1, enc_exp.size(1), -1, -1)\n",
        "        ], dim=-1)\n",
        "        return self.joiner(join).squeeze(2)"
      ],
      "metadata": {
        "id": "AmPJl7HInLf0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CTC Model Class\n",
        "class CTCModel(nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=128, vocab_size=len(vocab)):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.lstm(x)[0])"
      ],
      "metadata": {
        "id": "SriUtsBU1Yoh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute WER, CER, Edit Distance\n",
        "def compute_wer_cer(preds, targets, target_lens):\n",
        "    total_wer, total_cer, total_edit, offset = 0, 0, 0, 0\n",
        "    for i, p in enumerate(preds):\n",
        "        ref = targets[offset:offset + target_lens[i]].tolist()\n",
        "        offset += target_lens[i]\n",
        "        ref_str = ''.join(index_to_char[r] for r in ref)\n",
        "        hyp_str = ''.join(index_to_char[x] for x in p if x in index_to_char)\n",
        "        total_wer += wer(ref_str, hyp_str)\n",
        "        total_cer += cer(ref_str, hyp_str)\n",
        "        total_edit += editdistance.eval(ref_str, hyp_str)\n",
        "    n = len(preds)\n",
        "    return total_wer / n, total_cer / n, total_edit / n"
      ],
      "metadata": {
        "id": "E4U-b7ik5JvW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-level accuracy between predicted and target sequences\n",
        "def compute_char_accuracy(preds, targets, target_lens):\n",
        "    acc, total, offset = 0, 0, 0\n",
        "    for p, tlen in zip(preds, target_lens):\n",
        "        ref = targets[offset:offset + tlen].tolist()\n",
        "        offset += tlen\n",
        "        acc += sum(a == b for a, b in zip(p, ref))\n",
        "        total += max(len(ref), 1)\n",
        "    return acc / total if total > 0 else 0.0"
      ],
      "metadata": {
        "id": "ClzVUYm6nHqP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy decoding for character prediction from model output\n",
        "def greedy_decode(logits, input_lens, blank_index):\n",
        "    pred = logits.argmax(dim=2)\n",
        "    results = []\n",
        "    for i in range(pred.size(0)):\n",
        "        tokens, prev = [], None\n",
        "        for j in range(input_lens[i]):\n",
        "            idx = pred[i, j].item()\n",
        "            if idx != blank_index and idx != prev:\n",
        "                tokens.append(idx)\n",
        "            prev = idx\n",
        "        results.append(tokens)\n",
        "    return results"
      ],
      "metadata": {
        "id": "CtH_sjsInDa4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function for CTC model\n",
        "def train_ctc_model(model, loader, epochs=3):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CTCLoss(blank=char_to_index['|'], zero_infinity=True)\n",
        "    losses, accs = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = total_acc = 0\n",
        "        batches = 0\n",
        "        for specs, targets_flat, input_lens, target_lens in loader:\n",
        "            logits = model(specs)\n",
        "            log_probs = logits.log_softmax(2).transpose(0, 1)\n",
        "            loss = loss_fn(log_probs, targets_flat, input_lens, target_lens)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            preds = greedy_decode(logits, input_lens, char_to_index['|'])\n",
        "            acc = compute_char_accuracy(preds, targets_flat, target_lens)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc\n",
        "            batches += 1\n",
        "\n",
        "        avg_loss = total_loss / batches\n",
        "        avg_acc = total_acc / batches\n",
        "        losses.append(avg_loss)\n",
        "        accs.append(avg_acc)\n",
        "        torch.save(model.state_dict(), f\"/content/drive/MyDrive/ctc_epoch_{epoch+1}.pt\")\n",
        "        print(f\"[CTC Epoch {epoch+1}] Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}\")\n",
        "\n",
        "    return losses, accs"
      ],
      "metadata": {
        "id": "KmX9KIL03Qb2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function for RNN-T model\n",
        "def train_rnnt_model(model, loader, epochs=3):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CTCLoss(blank=char_to_index['|'], zero_infinity=True)\n",
        "    losses, accs, wers, cers, edits = [], [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = total_acc = total_wer = total_cer = total_edit = 0\n",
        "        batches = 0\n",
        "\n",
        "        for specs, targets_flat, input_lens, target_lens in loader:\n",
        "            B = specs.size(0)\n",
        "            max_len = target_lens.max().item()\n",
        "            decoder_inputs = torch.zeros(B, max_len, dtype=torch.long)\n",
        "            offset = 0\n",
        "            for i in range(B):\n",
        "                decoder_inputs[i, :target_lens[i]] = targets_flat[offset:offset + target_lens[i]]\n",
        "                offset += target_lens[i]\n",
        "\n",
        "            logits = model(specs, decoder_inputs).log_softmax(-1)\n",
        "            logits_ctc = logits.mean(dim=2).transpose(0, 1)\n",
        "            loss = loss_fn(logits_ctc, targets_flat, input_lens, target_lens)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            preds = greedy_decode(logits_ctc.transpose(0, 1), input_lens, char_to_index['|'])\n",
        "            acc = compute_char_accuracy(preds, targets_flat, target_lens)\n",
        "            w, c, e = compute_wer_cer(preds, targets_flat, target_lens)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += acc\n",
        "            total_wer += w\n",
        "            total_cer += c\n",
        "            total_edit += e\n",
        "            batches += 1\n",
        "\n",
        "        avg_loss = total_loss / batches\n",
        "        avg_acc = total_acc / batches\n",
        "        avg_wer = total_wer / batches\n",
        "        avg_cer = total_cer / batches\n",
        "        avg_edit = total_edit / batches\n",
        "\n",
        "        losses.append(avg_loss)\n",
        "        accs.append(avg_acc)\n",
        "        wers.append(avg_wer)\n",
        "        cers.append(avg_cer)\n",
        "        edits.append(avg_edit)\n",
        "        torch.save(model.state_dict(), f\"/content/drive/MyDrive/rnnt_epoch_{epoch+1}.pt\")\n",
        "        print(f\"[RNN-T Epoch {epoch+1}] Loss: {avg_loss:.4f}, Acc: {avg_acc:.4f}, WER: {avg_wer:.4f}\")\n",
        "\n",
        "    return losses, accs, wers, cers, edits"
      ],
      "metadata": {
        "id": "KP0liDFI3U1W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data_dir = \"/content/drive/MyDrive/TORGO\"\n",
        "\n",
        "# Load dataset\n",
        "dataset = TorgoDataset(data_dir, char_to_index)\n",
        "torch.save(dataset.items, '/content/drive/MyDrive/torgo_items.pt')\n",
        "#dataset = TorgoDataset(data_dir, char_to_index, items_file='/content/drive/MyDrive/torgo_items.pt')\n",
        "print(f\"Dataset length: {len(dataset)}\")\n",
        "\n",
        "# Calculate lengths\n",
        "total_len = len(dataset)\n",
        "train_len = int(0.7 * total_len)\n",
        "val_len = int(0.1 * total_len)\n",
        "test_len = total_len - train_len - val_len\n",
        "\n",
        "# Perform split\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtyxmwrT3T8E",
        "outputId": "8cafa507-4900-4906-87ff-8c746e73446b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Dataset (8224 files): 100%|██████████| 8224/8224 [44:48<00:00,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length: 8214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TorgoDataset(data_dir, char_to_index, items_file='/content/drive/MyDrive/torgo_items.pt')\n"
      ],
      "metadata": {
        "id": "m7YUvPYZ_iPd"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CTC model\n",
        "ctc_model = CTCModel()\n",
        "ctc_losses, ctc_accs = train_ctc_model(ctc_model, loader)"
      ],
      "metadata": {
        "id": "QDjqGXDa9QaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train RNN-T model\n",
        "rnnt_model = RNNTModel()\n",
        "rnnt_losses, rnnt_accs, rnnt_wers, rnnt_cers, rnnt_edits = train_rnnt_model(rnnt_model, loader)"
      ],
      "metadata": {
        "id": "BOs1T3K29QQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot CTC training loss and accuracy\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(ctc_losses, marker='o', label=\"CTC Loss\")\n",
        "plt.title(\"CTC Model - Loss over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(ctc_accs, marker='o', label=\"CTC Accuracy\", color='green')\n",
        "plt.title(\"CTC Model - Accuracy over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GNAfDr0I3nie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot RNN-T training metrics\n",
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(rnnt_losses, marker='o', label=\"RNN-T Loss\")\n",
        "plt.title(\"RNN-T Model - Loss over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(rnnt_accs, marker='o', label=\"RNN-T Accuracy\", color='green')\n",
        "plt.title(\"RNN-T Model - Accuracy over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "plt.plot(rnnt_wers, marker='o', label=\"WER\", color='red')\n",
        "plt.plot(rnnt_cers, marker='o', label=\"CER\", color='blue')\n",
        "plt.title(\"RNN-T Model - WER and CER over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Error Rate\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "plt.plot(rnnt_edits, marker='o', label=\"Edit Distance\", color='purple')\n",
        "plt.title(\"RNN-T Model - Edit Distance over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Edit Distance\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ewfLTD8M35r8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}