{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPrmpWk4YXNwfI7w+g5OA0P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/captainkeemo/Dysarthric-Speech-Transcription/blob/main/models/base_CTC_RNNT_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer editdistance --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt16kcAt1rQ5",
        "outputId": "984bc46e-ecda-490b-d105-7e4b9d3b3a5d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hpptgrn4DkLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf63eca-fece-4c99-e6bf-385478a25294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from jiwer import wer, cer\n",
        "import editdistance\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "CyEKx_H6q-hU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler_ctc = GradScaler()\n",
        "scaler_rnnt = GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rJaRsiH21Sv",
        "outputId": "2c7d03b8-764b-4c26-e173-89505605bf34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ae68ad2b3aa0>:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_ctc = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "<ipython-input-4-ae68ad2b3aa0>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_rnnt = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary\n",
        "vocab = list(\"abcdefghijklmnopqrstuvwxyz '\") + [\"|\"]\n",
        "char_to_index = {c: i for i, c in enumerate(vocab)}\n",
        "index_to_char = {i: c for c, i in char_to_index.items()}\n"
      ],
      "metadata": {
        "id": "ociJ8CjFruC-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_transcription(text):\n",
        "    text = text.lower().strip()\n",
        "    if '.jpg' in text or '[say' in text or text == 'xxx' or text == '':\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "def decode_sequence(indices, index_to_char):\n",
        "    return ''.join([index_to_char.get(i, '') for i in indices]).replace('|', '').strip()"
      ],
      "metadata": {
        "id": "THjnexGQaZk1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the device\n",
        "def get_device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "KmvJJFuWizAY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Mel spectrograms\n",
        "def preprocess_and_save_mel(root_dir, save_dir, sample_rate=16000, n_mels=80, use_gpu=False):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels).to(device)\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=sample_rate).to(device)\n",
        "\n",
        "    # Step 1: Gather .wav files with tqdm\n",
        "    print(\"Scanning for .wav files...\")\n",
        "    wav_paths = []\n",
        "    for dirpath, _, filenames in tqdm(os.walk(root_dir), desc=\"Scanning folders\"):\n",
        "        for fname in filenames:\n",
        "            if fname.endswith('.wav') and 'wav_headMic' in dirpath:\n",
        "                wav_paths.append(os.path.join(dirpath, fname))\n",
        "\n",
        "    print(f\"Found {len(wav_paths)} .wav files. Starting preprocessing...\")\n",
        "\n",
        "    total_saved = 0\n",
        "\n",
        "    # Step 2: Preprocess with tqdm\n",
        "    for wav_path in tqdm(wav_paths, desc=\"Preprocessing Mel Spectrograms\"):\n",
        "        mel_path = os.path.join(save_dir, os.path.basename(wav_path).replace('.wav', '.pt'))\n",
        "        txt_path = wav_path.replace('wav_headMic', 'prompts').replace('.wav', '.txt')\n",
        "\n",
        "        if not os.path.exists(txt_path) or os.path.exists(mel_path):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            waveform, sr = torchaudio.load(wav_path)\n",
        "            waveform = waveform.to(device)\n",
        "\n",
        "            if sr != sample_rate:\n",
        "                waveform = resampler(waveform)\n",
        "\n",
        "            mel_spec = mel_transform(waveform).squeeze(0).transpose(0, 1).cpu()\n",
        "\n",
        "            with open(txt_path, 'r', encoding='utf-8') as f:\n",
        "                transcript = f.read().strip().lower()\n",
        "\n",
        "            if '.jpg' in transcript or transcript in ('xxx', '') or '[say' in transcript:\n",
        "                continue\n",
        "\n",
        "            torch.save((mel_spec, transcript), mel_path)\n",
        "            total_saved += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {wav_path}: {e}\")\n",
        "\n",
        "    print(f\"\\nDone: {total_saved} mel .pt files saved to {save_dir}\")\n"
      ],
      "metadata": {
        "id": "hz_eW9R0oUwD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "preprocess_and_save_mel(\n",
        "    root_dir=\"/content/drive/MyDrive/TORGO\",\n",
        "    save_dir=\"/content/drive/MyDrive/TORGO_mel_preprocessed\"\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "RZt0Hj-SoZ1E",
        "outputId": "e945b3fa-319e-41a1-d2de-532dcce0344a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npreprocess_and_save_mel(\\n    root_dir=\"/content/drive/MyDrive/TORGO\",\\n    save_dir=\"/content/drive/MyDrive/TORGO_mel_preprocessed\"\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TorgoDataset(Dataset):\n",
        "    def __init__(self, root_dir, char_to_index, sample_rate=16000,\n",
        "                 items_file=None, use_precomputed=False, mel_dir=None):\n",
        "        self.char_to_index = char_to_index\n",
        "        self.sample_rate = sample_rate\n",
        "        self.use_precomputed = use_precomputed\n",
        "        self.items = []\n",
        "\n",
        "        if self.use_precomputed:\n",
        "            assert mel_dir is not None, \"If using precomputed features, you must provide mel_dir.\"\n",
        "            self.paths = sorted([\n",
        "                os.path.join(mel_dir, f)\n",
        "                for f in os.listdir(mel_dir) if f.endswith('.pt')\n",
        "            ])\n",
        "        else:\n",
        "            self.mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=80)\n",
        "            self.resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=sample_rate)\n",
        "\n",
        "            if items_file is not None:\n",
        "                loaded_items = torch.load(items_file)\n",
        "                for wav_fp, text in loaded_items:\n",
        "                    cleaned = clean_transcription(text)\n",
        "                    if cleaned:\n",
        "                        self.items.append((wav_fp, cleaned))\n",
        "            else:\n",
        "                wav_files = []\n",
        "                for dirpath, _, filenames in os.walk(root_dir):\n",
        "                    for fname in filenames:\n",
        "                        if fname.endswith('.wav') and 'wav_headMic' in dirpath:\n",
        "                            wav_files.append(os.path.join(dirpath, fname))\n",
        "\n",
        "                for wav_fp in tqdm(wav_files, desc=f\"Building dataset from .wav files ({len(wav_files)} found)\"):\n",
        "                    txt_fp = wav_fp.replace('wav_headMic', 'prompts').replace('.wav', '.txt')\n",
        "                    if not os.path.exists(txt_fp):\n",
        "                        continue\n",
        "                    with open(txt_fp, 'r', encoding='utf-8') as f:\n",
        "                        text = f.read()\n",
        "                    cleaned = clean_transcription(text)\n",
        "                    if cleaned:\n",
        "                        self.items.append((wav_fp, cleaned))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths) if self.use_precomputed else len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.use_precomputed:\n",
        "            mel_path = self.paths[idx]\n",
        "            mel_spec, transcript = torch.load(mel_path)\n",
        "        else:\n",
        "            wav_path, transcript = self.items[idx]\n",
        "            waveform, sr = torchaudio.load(wav_path)\n",
        "            if sr != self.sample_rate:\n",
        "                waveform = self.resampler(waveform)\n",
        "            mel_spec = self.mel_transform(waveform).squeeze(0).transpose(0, 1)\n",
        "\n",
        "        target = torch.tensor([self.char_to_index[c] for c in transcript if c in self.char_to_index], dtype=torch.long)\n",
        "        return mel_spec, target\n"
      ],
      "metadata": {
        "id": "NF-gwWrBq6zj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    features, targets = zip(*batch)\n",
        "\n",
        "    # Compute original lengths\n",
        "    input_lengths = torch.tensor([feat.size(0) for feat in features], dtype=torch.long)\n",
        "    target_lengths = torch.tensor([len(tgt) for tgt in targets], dtype=torch.long)\n",
        "\n",
        "    # Pad features (time dimension is dim 0)\n",
        "    padded_features = pad_sequence(features, batch_first=True)  # shape: [B, T, F]\n",
        "\n",
        "    # Pad targets for inspection (not used in loss)\n",
        "    padded_targets = pad_sequence(targets, batch_first=True, padding_value=0)\n",
        "\n",
        "    return padded_features, input_lengths, padded_targets, target_lengths\n"
      ],
      "metadata": {
        "id": "IVPwDf-um_OJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the full RNN-T model with encoder, decoder, and joiner\n",
        "class RNNTModel(nn.Module):\n",
        "    def __init__(self, input_dim=80, vocab_size=len(vocab), hidden_dim=128, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.decoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.joiner = nn.Linear(hidden_dim * 2 + hidden_dim, vocab_size)  # enc (bi) + dec (uni)\n",
        "\n",
        "    def forward(self, encoder_input, target_input):\n",
        "        # encoder_input: [B, T_enc, 80]\n",
        "        # target_input: [B, T_dec]\n",
        "        enc_out, _ = self.encoder(encoder_input)  # [B, T_enc, 2*H]\n",
        "        dec_emb = self.embed(target_input)        # [B, T_dec, E]\n",
        "        dec_out, _ = self.decoder(dec_emb)        # [B, T_dec, H]\n",
        "\n",
        "        # Expand for joiner broadcasting\n",
        "        enc_exp = enc_out.unsqueeze(2)            # [B, T_enc, 1, 2H]\n",
        "        dec_exp = dec_out.unsqueeze(1)            # [B, 1, T_dec, H]\n",
        "\n",
        "        # Joiner: concat and project\n",
        "        enc_b, t_enc, _, h_enc = enc_exp.shape\n",
        "        dec_b, _, t_dec, h_dec = dec_exp.shape\n",
        "        assert enc_b == dec_b\n",
        "\n",
        "        enc_rep = enc_exp.expand(-1, t_enc, t_dec, -1)  # [B, T_enc, T_dec, 2H]\n",
        "        dec_rep = dec_exp.expand(-1, t_enc, t_dec, -1)  # [B, T_enc, T_dec, H]\n",
        "\n",
        "        joined = torch.cat([enc_rep, dec_rep], dim=-1)  # [B, T_enc, T_dec, 3H]\n",
        "        logits = self.joiner(joined)                    # [B, T_enc, T_dec, V]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "AmPJl7HInLf0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CTC Model Class\n",
        "class CTCModel(nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=128, vocab_size=len(vocab), num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=num_layers, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.lstm(x)\n",
        "        logits = self.fc(x)\n",
        "        return logits  # shape: [B, T, vocab_size]\n"
      ],
      "metadata": {
        "id": "SriUtsBU1Yoh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BeamSearch:\n",
        "    def __init__(self, beam_width, vocab_size, blank_index):\n",
        "        self.beam_width = beam_width\n",
        "        self.vocab_size = vocab_size\n",
        "        self.blank_index = blank_index\n",
        "\n",
        "    def decode(self, logits):\n",
        "        batch_size = logits.size(1)\n",
        "        T = logits.size(0)\n",
        "        results = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            beams = [([], 0.0)]\n",
        "            for t in range(T):\n",
        "                new_beams = []\n",
        "                log_probs = F.log_softmax(logits[t, b], dim=-1)\n",
        "                topk = torch.topk(log_probs, self.beam_width)\n",
        "\n",
        "                for seq, score in beams:\n",
        "                    for i in range(self.beam_width):\n",
        "                        token = topk.indices[i].item()\n",
        "                        new_score = score + topk.values[i].item()\n",
        "                        if token == self.blank_index:\n",
        "                            new_beams.append((seq, new_score))\n",
        "                        else:\n",
        "                            new_beams.append((seq + [token], new_score))\n",
        "\n",
        "                new_beams.sort(key=lambda x: x[1], reverse=True)\n",
        "                beams = new_beams[:self.beam_width]\n",
        "\n",
        "            best_seq = max(beams, key=lambda x: x[1])[0]\n",
        "            results.append(best_seq)  # Changed from results.append([best_seq])\n",
        "\n",
        "        # Ensure results is always a list of lists even with beam_width=1\n",
        "        results = [[item] if isinstance(item, int) else item for item in results]\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "5t3s_kBfeuMl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute WER, CER, Edit Distance\n",
        "from torchaudio.functional import edit_distance as torchaudio_edit_distance\n",
        "from jiwer import wer, cer\n",
        "\n",
        "def compute_wer_cer(preds, input_lens, targets, target_lens, index_to_char):\n",
        "    total_wer, total_cer = 0.0, 0.0\n",
        "    offset = 0\n",
        "    valid_examples = 0\n",
        "\n",
        "    for i in range(len(preds)):\n",
        "        tlen = target_lens[i].item()\n",
        "        if tlen == 0:\n",
        "            continue  # skip empty targets\n",
        "\n",
        "        target_slice = targets[offset:offset + tlen]\n",
        "        target_seq = target_slice.view(-1).tolist()\n",
        "        offset += tlen\n",
        "\n",
        "        pred_seq = preds[i] if isinstance(preds[i], (list, tuple)) else [preds[i]]\n",
        "        pred_str = ''.join([index_to_char.get(p, '') for p in pred_seq])\n",
        "        target_str = ''.join([index_to_char.get(t, '') for t in target_seq])\n",
        "\n",
        "        if not target_str.strip():\n",
        "            continue  # skip if the reference string is still empty\n",
        "\n",
        "        total_wer += wer(target_str, pred_str)\n",
        "        total_cer += cer(target_str, pred_str)\n",
        "        valid_examples += 1\n",
        "\n",
        "    if valid_examples == 0:\n",
        "        return 1.0, 1.0  # fallback if no valid examples\n",
        "    return total_wer / valid_examples, total_cer / valid_examples\n",
        "\n"
      ],
      "metadata": {
        "id": "E4U-b7ik5JvW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jiwer import cer\n",
        "\n",
        "def decode_indices(indices, index_to_char):\n",
        "    return ''.join([index_to_char[i] for i in indices if i in index_to_char]).replace('|', ' ').strip()\n",
        "\n",
        "def compute_cer_accuracy(preds, targets, target_lens, index_to_char):\n",
        "    offset = 0\n",
        "    all_refs, all_hyps = [], []\n",
        "    for i, tlen in enumerate(target_lens):\n",
        "        ref = ''.join([index_to_char[x.item()] for x in targets[offset:offset + tlen]])\n",
        "        hyp = ''.join([index_to_char[x] for x in preds[i]])\n",
        "        all_refs.append(ref)\n",
        "        all_hyps.append(hyp)\n",
        "        offset += tlen\n",
        "    return 1.0 - cer(all_refs, all_hyps)\n"
      ],
      "metadata": {
        "id": "ClzVUYm6nHqP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy decoding for character prediction from model output\n",
        "def greedy_decode(logits, input_lens, blank_index):\n",
        "    pred = logits.argmax(dim=2)\n",
        "    results = []\n",
        "    for i in range(pred.size(0)):\n",
        "        tokens, prev = [], None\n",
        "        for j in range(input_lens[i]):\n",
        "            idx = pred[i, j].item()\n",
        "            if idx != blank_index and idx != prev:\n",
        "                tokens.append(idx)\n",
        "            prev = idx\n",
        "        results.append(tokens)\n",
        "    return results"
      ],
      "metadata": {
        "id": "CtH_sjsInDa4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_ctc(model, train_loader, val_loader, optimizer, loss_fn, index_to_char,\n",
        "              device, epochs=20, patience=5, blank_idx=1, beam_width=5):\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    wait = 0\n",
        "\n",
        "    train_loss_hist, val_loss_hist = [], []\n",
        "    train_wer_hist, val_wer_hist = [], []\n",
        "    train_cer_hist, val_cer_hist = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        model.train()\n",
        "        train_loss, train_wer, train_cer = 0, 0, 0\n",
        "\n",
        "        train_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "        for feats, feat_lens, targets, target_lens in train_bar:\n",
        "            feats, feat_lens = feats.to(device), feat_lens.to(device)\n",
        "            targets, target_lens = targets.to(device), target_lens.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(feats)\n",
        "            log_probs = logits.log_softmax(2).transpose(0, 1)\n",
        "            loss = loss_fn(log_probs, targets, feat_lens, target_lens)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                decoder = BeamSearch(beam_width, logits.size(2), blank_idx)\n",
        "                raw_preds = decoder.decode(log_probs.cpu())\n",
        "\n",
        "                preds = [beam[0] if isinstance(beam[0], (list, tuple)) else beam for beam in raw_preds]\n",
        "                preds = [p if isinstance(p, list) else [p] for p in preds]\n",
        "\n",
        "                wer_score, cer_score = compute_wer_cer(preds, feat_lens.cpu(), targets.cpu(), target_lens.cpu(), index_to_char)\n",
        "                train_wer += wer_score\n",
        "                train_cer += cer_score\n",
        "\n",
        "            train_bar.set_postfix(loss=loss.item(), WER=wer_score, CER=cer_score)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_wer /= len(train_loader)\n",
        "        train_cer /= len(train_loader)\n",
        "\n",
        "        # --- Validation ---\n",
        "        model.eval()\n",
        "        val_loss, val_wer, val_cer = 0, 0, 0\n",
        "        val_bar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for feats, feat_lens, targets, target_lens in val_bar:\n",
        "                feats, feat_lens = feats.to(device), feat_lens.to(device)\n",
        "                targets, target_lens = targets.to(device), target_lens.to(device)\n",
        "\n",
        "                logits = model(feats)\n",
        "                log_probs = logits.log_softmax(2).transpose(0, 1)\n",
        "                loss = loss_fn(log_probs, targets, feat_lens, target_lens)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                decoder = BeamSearch(beam_width, logits.size(2), blank_idx)\n",
        "                raw_preds = decoder.decode(log_probs.cpu())\n",
        "\n",
        "                preds = [beam[0] if isinstance(beam[0], (list, tuple)) else beam for beam in raw_preds]\n",
        "                preds = [p if isinstance(p, list) else [p] for p in preds]\n",
        "\n",
        "                wer_score, cer_score = compute_wer_cer(preds, feat_lens.cpu(), targets.cpu(), target_lens.cpu(), index_to_char)\n",
        "                val_wer += wer_score\n",
        "                val_cer += cer_score\n",
        "\n",
        "                val_bar.set_postfix(loss=loss.item(), WER=wer_score, CER=cer_score)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_wer /= len(val_loader)\n",
        "        val_cer /= len(val_loader)\n",
        "\n",
        "        train_loss_hist.append(train_loss)\n",
        "        val_loss_hist.append(val_loss)\n",
        "        train_wer_hist.append(train_wer)\n",
        "        val_wer_hist.append(val_wer)\n",
        "        train_cer_hist.append(train_cer)\n",
        "        val_cer_hist.append(val_cer)\n",
        "\n",
        "        print(f\"[CTC] Epoch {epoch+1}: \"\n",
        "              f\"Train Loss={train_loss:.4f}, WER={train_wer:.4f}, CER={train_cer:.4f} | \"\n",
        "              f\"Val Loss={val_loss:.4f}, WER={val_wer:.4f}, CER={val_cer:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_ctc_model.pt')\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    return train_loss_hist, val_loss_hist, train_wer_hist, val_wer_hist, train_cer_hist, val_cer_hist\n"
      ],
      "metadata": {
        "id": "KmX9KIL03Qb2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_rnnt(model, train_loader, val_loader, optimizer, loss_fn, index_to_char,\n",
        "               device, epochs=20, patience=5, blank_idx=1, beam_width=5):\n",
        "\n",
        "    model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    wait = 0\n",
        "\n",
        "    train_loss_hist, val_loss_hist = [], []\n",
        "    train_wer_hist, val_wer_hist = [], []\n",
        "    train_cer_hist, val_cer_hist = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss, train_wer, train_cer = 0, 0, 0\n",
        "\n",
        "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training (RNN-T)\")\n",
        "        for feats, feat_lens, targets, target_lens in train_pbar:\n",
        "            feats, feat_lens = feats.to(device), feat_lens.to(device)\n",
        "            targets, target_lens = targets.to(device), target_lens.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(feats, targets)\n",
        "            T_enc = logits.shape[1]\n",
        "            T_dec = logits.shape[2]\n",
        "            min_len = min(T_enc, T_dec, feat_lens.max().item())\n",
        "            logits = logits[:, torch.arange(min_len), torch.arange(min_len), :]\n",
        "            log_probs = logits.log_softmax(2).transpose(0, 1)\n",
        "            feat_lens = torch.clamp(feat_lens, max=log_probs.shape[0])\n",
        "\n",
        "            loss = loss_fn(log_probs, targets, feat_lens, target_lens)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                decoder = BeamSearch(beam_width, logits.size(2), blank_idx)\n",
        "                preds = decoder.decode(log_probs.cpu())\n",
        "                wer_score, cer_score = compute_wer_cer(preds, feat_lens.cpu(), targets.cpu(), target_lens.cpu(), index_to_char)\n",
        "                train_wer += wer_score\n",
        "                train_cer += cer_score\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_wer /= len(train_loader)\n",
        "        train_cer /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss, val_wer, val_cer = 0, 0, 0\n",
        "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} - Validation (RNN-T)\")\n",
        "        with torch.no_grad():\n",
        "            for feats, feat_lens, targets, target_lens in val_pbar:\n",
        "                feats, feat_lens = feats.to(device), feat_lens.to(device)\n",
        "                targets, target_lens = targets.to(device), target_lens.to(device)\n",
        "\n",
        "                logits = model(feats, targets)\n",
        "                T_enc = logits.shape[1]\n",
        "                T_dec = logits.shape[2]\n",
        "                min_len = min(T_enc, T_dec, feat_lens.max().item())\n",
        "                logits = logits[:, torch.arange(min_len), torch.arange(min_len), :]\n",
        "                log_probs = logits.log_softmax(2).transpose(0, 1)\n",
        "                feat_lens = torch.clamp(feat_lens, max=log_probs.shape[0])\n",
        "\n",
        "                loss = loss_fn(log_probs, targets, feat_lens, target_lens)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                decoder = BeamSearch(beam_width, logits.size(2), blank_idx)\n",
        "                preds = decoder.decode(log_probs.cpu())\n",
        "                wer_score, cer_score = compute_wer_cer(preds, feat_lens.cpu(), targets.cpu(), target_lens.cpu(), index_to_char)\n",
        "                val_wer += wer_score\n",
        "                val_cer += cer_score\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_wer /= len(val_loader)\n",
        "        val_cer /= len(val_loader)\n",
        "\n",
        "        train_loss_hist.append(train_loss)\n",
        "        val_loss_hist.append(val_loss)\n",
        "        train_wer_hist.append(train_wer)\n",
        "        val_wer_hist.append(val_wer)\n",
        "        train_cer_hist.append(train_cer)\n",
        "        val_cer_hist.append(val_cer)\n",
        "\n",
        "        print(f\"[RNN-T] Epoch {epoch+1}: Train Loss={train_loss:.4f}, WER={train_wer:.4f}, CER={train_cer:.4f} | \"\n",
        "              f\"Val Loss={val_loss:.4f}, WER={val_wer:.4f}, CER={val_cer:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_rnnt_model.pt')\n",
        "            wait = 0\n",
        "        else:\n",
        "            wait += 1\n",
        "            if wait >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    return train_loss_hist, val_loss_hist, train_wer_hist, val_wer_hist, train_cer_hist, val_cer_hist\n"
      ],
      "metadata": {
        "id": "KP0liDFI3U1W"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_and_plot_model(model, test_loader, loss_fn, index_to_char, device, blank_idx=1,\n",
        "                            use_beam=True, beam_width=5, model_type=\"ctc\"):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    total_wer = 0\n",
        "    total_cer = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for feats, feat_lens, targets, target_lens in test_loader:\n",
        "            feats, feat_lens = feats.to(device), feat_lens.to(device)\n",
        "            targets, target_lens = targets.to(device), target_lens.to(device)\n",
        "\n",
        "            logits = model(feats)\n",
        "            log_probs = logits.log_softmax(2).transpose(0, 1)\n",
        "            loss = loss_fn(log_probs, targets, feat_lens, target_lens)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            if use_beam:\n",
        "                decoder = BeamSearch(beam_width, logits.size(-1), blank_idx)\n",
        "                preds = decoder.decode(log_probs.cpu())\n",
        "            else:\n",
        "                preds = greedy_decode(logits.cpu(), feat_lens.cpu(), blank_idx)\n",
        "\n",
        "            wer_score, cer_score = compute_wer_cer(preds, feat_lens.cpu(), targets.cpu(), target_lens.cpu(), index_to_char)\n",
        "            total_wer += wer_score\n",
        "            total_cer += cer_score\n",
        "\n",
        "    avg_loss = test_loss / len(test_loader)\n",
        "    avg_wer = total_wer / len(test_loader)\n",
        "    avg_cer = total_cer / len(test_loader)\n",
        "\n",
        "    print(f\"[Test - {model_type.upper()}] Loss: {avg_loss:.4f} | WER: {avg_wer:.4f} | CER: {avg_cer:.4f}\")\n",
        "    return avg_loss, avg_wer, avg_cer\n"
      ],
      "metadata": {
        "id": "1HzSEUfL_WS9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data_dir = \"/content/drive/MyDrive/TORGO\"\n",
        "\n",
        "# Load dataset\n",
        "#dataset = TorgoDataset(data_dir, char_to_index)\n",
        "#torch.save(dataset.items, '/content/drive/MyDrive/torgo_items.pt')\n",
        "dataset = TorgoDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/TORGO\",  # still required for structure\n",
        "    char_to_index=char_to_index,\n",
        "    use_precomputed=True,\n",
        "    mel_dir=\"/content/drive/MyDrive/TORGO_mel_preprocessed\"\n",
        ")\n",
        "print(f\"Dataset length: {len(dataset)}\")\n",
        "\n",
        "# Calculate lengths\n",
        "total_len = len(dataset)\n",
        "train_len = int(0.7 * total_len)\n",
        "val_len = int(0.1 * total_len)\n",
        "test_len = total_len - train_len - val_len\n",
        "\n",
        "# Perform split\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtyxmwrT3T8E",
        "outputId": "f85a0acf-d848-424f-c542-31090fa3725d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset length: 993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CTC model\n",
        "model_ctc = CTCModel()\n",
        "optimizer_ctc = torch.optim.Adam(model_ctc.parameters(), lr=1e-5)\n",
        "loss_fn_ctc = nn.CTCLoss(blank=char_to_index['|'], zero_infinity=True)\n",
        "device = get_device()\n",
        "\n",
        "ctc_train_loss, ctc_val_loss, ctc_train_wer, ctc_val_wer, ctc_train_cer, ctc_val_cer = train_ctc(\n",
        "    model=model_ctc,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer_ctc,\n",
        "    loss_fn=loss_fn_ctc,\n",
        "    index_to_char=index_to_char,\n",
        "    device=device\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "QDjqGXDa9QaN",
        "outputId": "c9273a5f-4aaf-4fb2-d438-407f019b76ac"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-bc631f079664>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m ctc_train_loss, ctc_val_loss, ctc_train_wer, ctc_val_wer, ctc_train_cer, ctc_val_cer = train_ctc(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_ctc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-b36bea522da5>\u001b[0m in \u001b[0;36mtrain_ctc\u001b[0;34m(model, train_loader, val_loader, optimizer, loss_fn, index_to_char, device, epochs, patience, blank_idx, beam_width)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f9b927c96c37>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_precomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mmel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mmel_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mwav_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m             \u001b[0;31m# If we want to actually tail call to torch.jit.load, we need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_is_zipfile\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# Read the first few bytes and match against the ZIP file signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0mlocal_header_magic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0mread_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_header_magic_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mread_bytes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlocal_header_magic_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train RNN-T model\n",
        "model_rnnt = RNNTModel()\n",
        "optimizer_rnnt = torch.optim.Adam(model_rnnt.parameters(), lr=1e-4)\n",
        "loss_fn_rnnt = nn.CTCLoss(blank=char_to_index['|'], zero_infinity=True)\n",
        "device = get_device()\n",
        "\n",
        "rnnt_train_loss, rnnt_val_loss, rnnt_train_wer, rnnt_val_wer, rnnt_train_cer, rnnt_val_cer = train_rnnt(\n",
        "    model=model_rnnt,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer_rnnt,\n",
        "    loss_fn=loss_fn_rnnt,\n",
        "    index_to_char=index_to_char,\n",
        "    epochs=30,\n",
        "    patience=5,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "BOs1T3K29QQ1",
        "outputId": "77917407-75f9-4748-f5ff-5bcc37de19d0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 - Training (RNN-T):  27%|██▋       | 3/11 [00:13<00:35,  4.43s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-9ce7c8d6ca70>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m rnn_train_loss, rnn_val_loss, rnn_train_wer, rnn_val_wer, rnn_train_cer, rnn_val_cer = train_rnnt(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_rnnt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-119216bcf20b>\u001b[0m in \u001b[0;36mtrain_rnnt\u001b[0;34m(model, train_loader, val_loader, optimizer, loss_fn, index_to_char, device, epochs, patience, blank_idx, beam_width)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mT_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mT_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-bb684f778abc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_input, target_input)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdec_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_exp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, T_enc, T_dec, H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mjoined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menc_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_rep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [B, T_enc, T_dec, 3H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoiner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoined\u001b[0m\u001b[0;34m)\u001b[0m                    \u001b[0;31m# [B, T_enc, T_dec, V]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctc_results = evaluate_and_plot_model(\n",
        "    model=model_ctc,\n",
        "    test_loader=test_loader,\n",
        "    loss_fn=loss_fn_ctc,\n",
        "    blank_index=char_to_index['|'],\n",
        "    index_to_char=index_to_char,\n",
        "    train_losses=ctc_train_loss,\n",
        "    val_losses=ctc_val_loss,\n",
        "    train_accuracies=ctc_train_acc,\n",
        "    val_accuracies=ctc_val_acc,\n",
        "    model_name=\"CTC\",\n",
        "    device='cuda'\n",
        ")"
      ],
      "metadata": {
        "id": "GNAfDr0I3nie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnnt_results = evaluate_and_plot_model(\n",
        "    model=model_rnnt,\n",
        "    test_loader=test_loader,\n",
        "    loss_fn=loss_fn_rnnt,\n",
        "    blank_index=char_to_index['|'],\n",
        "    index_to_char=index_to_char,\n",
        "    train_losses=rnnt_train_loss,\n",
        "    val_losses=rnnt_val_loss,\n",
        "    train_accuracies=rnnt_train_acc,\n",
        "    val_accuracies=rnnt_val_acc,\n",
        "    model_name=\"RNN-T\",\n",
        "    device='cuda'\n",
        ")"
      ],
      "metadata": {
        "id": "ewfLTD8M35r8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}