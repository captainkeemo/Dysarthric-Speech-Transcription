# -*- coding: utf-8 -*-
"""base_CTC_RNNT_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N4h866wMWWdHW0-46L2e8yXJKbSKMSEd
"""

!pip install jiwer editdistance --quiet

from google.colab import drive
drive.mount('/content/drive')

import os
import torch
import torchaudio
import torch.nn as nn
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from jiwer import wer, cer
import editdistance
import glob
from tqdm import tqdm
import torch.nn.functional as F
from tqdm.auto import tqdm
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import random_split

from torch.cuda.amp import autocast, GradScaler
scaler_ctc = GradScaler()
scaler_rnnt = GradScaler()

# Define vocabulary
vocab = list("abcdefghijklmnopqrstuvwxyz '") + ["|"]
char_to_index = {c: i for i, c in enumerate(vocab)}
index_to_char = {i: c for c, i in char_to_index.items()}

# Function to get the device
def get_device():
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Preprocessing Mel spectrograms
def preprocess_and_save_mel(root_dir, save_dir, sample_rate=16000, n_mels=80, use_gpu=False):
    os.makedirs(save_dir, exist_ok=True)

    device = torch.device("cuda" if use_gpu and torch.cuda.is_available() else "cpu")
    mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels).to(device)
    resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=sample_rate).to(device)

    # Step 1: Gather .wav files with tqdm
    print("Scanning for .wav files...")
    wav_paths = []
    for dirpath, _, filenames in tqdm(os.walk(root_dir), desc="Scanning folders"):
        for fname in filenames:
            if fname.endswith('.wav') and 'wav_headMic' in dirpath:
                wav_paths.append(os.path.join(dirpath, fname))

    print(f"Found {len(wav_paths)} .wav files. Starting preprocessing...")

    total_saved = 0

    # Step 2: Preprocess with tqdm
    for wav_path in tqdm(wav_paths, desc="Preprocessing Mel Spectrograms"):
        mel_path = os.path.join(save_dir, os.path.basename(wav_path).replace('.wav', '.pt'))
        txt_path = wav_path.replace('wav_headMic', 'prompts').replace('.wav', '.txt')

        if not os.path.exists(txt_path) or os.path.exists(mel_path):
            continue

        try:
            waveform, sr = torchaudio.load(wav_path)
            waveform = waveform.to(device)

            if sr != sample_rate:
                waveform = resampler(waveform)

            mel_spec = mel_transform(waveform).squeeze(0).transpose(0, 1).cpu()

            with open(txt_path, 'r', encoding='utf-8') as f:
                transcript = f.read().strip().lower()

            if '.jpg' in transcript or transcript in ('xxx', '') or '[say' in transcript:
                continue

            torch.save((mel_spec, transcript), mel_path)
            total_saved += 1
        except Exception as e:
            print(f"Error processing {wav_path}: {e}")

    print(f"\nDone: {total_saved} mel .pt files saved to {save_dir}")

'''
preprocess_and_save_mel(
    root_dir="/content/drive/MyDrive/TORGO",
    save_dir="/content/drive/MyDrive/TORGO_mel_preprocessed"
)
'''

class TorgoDataset(Dataset):
    def __init__(self, root_dir, char_to_index, sample_rate=16000, items_file=None, use_precomputed=False, mel_dir=None):
        self.char_to_index = char_to_index
        self.sample_rate = sample_rate
        self.use_precomputed = use_precomputed
        self.items = []

        if self.use_precomputed:
            assert mel_dir is not None, "If using precomputed features, you must provide mel_dir."
            self.paths = sorted([
                os.path.join(mel_dir, f)
                for f in os.listdir(mel_dir) if f.endswith('.pt')
            ])
        else:
            self.mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_mels=80)
            self.resampler = torchaudio.transforms.Resample(orig_freq=48000, new_freq=sample_rate)

            if items_file is not None:
                # Load cached wav+text metadata, then re-clean
                loaded_items = torch.load(items_file)
                for wav_fp, transcription_text in loaded_items:
                    transcription_text = transcription_text.strip().lower()
                    if '.jpg' in transcription_text or transcription_text in ('xxx', '') or '[say' in transcription_text:
                        continue
                    self.items.append((wav_fp, transcription_text))
            else:
                wav_files = []
                for dirpath, _, filenames in os.walk(root_dir):
                    for fname in filenames:
                        if fname.endswith('.wav') and 'wav_headMic' in dirpath:
                            wav_files.append(os.path.join(dirpath, fname))

                for wav_fp in tqdm(wav_files, desc=f"Building dataset from .wav files ({len(wav_files)} found)"):
                    txt_fp = wav_fp.replace('wav_headMic', 'prompts').replace('.wav', '.txt')
                    if not os.path.exists(txt_fp):
                        continue
                    with open(txt_fp, 'r', encoding='utf-8') as f:
                        transcription_text = f.read().strip().lower()
                    if '.jpg' in transcription_text or transcription_text in ('xxx', '') or '[say' in transcription_text:
                        continue
                    self.items.append((wav_fp, transcription_text))

    def __len__(self):
        return len(self.paths) if self.use_precomputed else len(self.items)

    def __getitem__(self, idx):
        if self.use_precomputed:
            mel_path = self.paths[idx]
            mel_spec, transcript = torch.load(mel_path)
        else:
            wav_path, transcript = self.items[idx]
            waveform, sr = torchaudio.load(wav_path)
            if sr != self.sample_rate:
                waveform = self.resampler(waveform)
            mel_spec = self.mel_transform(waveform).squeeze(0).transpose(0, 1)

        target = torch.tensor([self.char_to_index[c] for c in transcript if c in self.char_to_index])
        return mel_spec, target

# Batch collation for DataLoader: pad inputs and concatenate targets
def collate_fn(batch):
    # Filter out None samples
    batch = [sample for sample in batch if sample is not None]
    if len(batch) == 0:
        return None
    specs, targets = zip(*batch)
    spec_lens = torch.tensor([s.size(0) for s in specs], dtype=torch.long)
    target_lens = torch.tensor([t.size(0) for t in targets], dtype=torch.long)
    specs_padded = pad_sequence(specs, batch_first=True)
    targets_flat = torch.cat(targets)
    return specs_padded, targets_flat, spec_lens, target_lens

# Define the full RNN-T model with encoder, decoder, and joiner
class RNNTModel(nn.Module):
    def __init__(self, input_dim=80, vocab_size=len(vocab), hidden_dim=128, embed_dim=64):
        super().__init__()
        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.embed = nn.Embedding(vocab_size, embed_dim)
        self.decoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.joiner = nn.Linear(hidden_dim * 3, vocab_size)

    def forward(self, encoder_input, target_input):
        enc_out, _ = self.encoder(encoder_input)
        dec_emb = self.embed(target_input)
        dec_out, _ = self.decoder(dec_emb)
        enc_exp = enc_out.unsqueeze(2)
        dec_exp = dec_out.unsqueeze(1)
        join = torch.cat([
            enc_exp.expand(-1, -1, dec_exp.size(2), -1),
            dec_exp.expand(-1, enc_exp.size(1), -1, -1)
        ], dim=-1)
        return self.joiner(join).squeeze(2)

# CTC Model Class
class CTCModel(nn.Module):
    def __init__(self, input_dim=80, hidden_dim=128, vocab_size=len(vocab)):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, vocab_size)

    def forward(self, x):
        return self.fc(self.lstm(x)[0])

# Compute WER, CER, Edit Distance
def compute_wer_cer(preds, targets, target_lens):
    total_wer, total_cer, total_edit, offset = 0, 0, 0, 0
    for i, p in enumerate(preds):
        ref = targets[offset:offset + target_lens[i]].tolist()
        offset += target_lens[i]
        ref_str = ''.join(index_to_char[r] for r in ref)
        hyp_str = ''.join(index_to_char[x] for x in p if x in index_to_char)
        total_wer += wer(ref_str, hyp_str)
        total_cer += cer(ref_str, hyp_str)
        total_edit += editdistance.eval(ref_str, hyp_str)
    n = len(preds)
    return total_wer / n, total_cer / n, total_edit / n

from jiwer import cer

def decode_indices(indices, index_to_char):
    return ''.join([index_to_char[i] for i in indices if i in index_to_char]).replace('|', ' ').strip()

def compute_cer_accuracy(preds, targets, target_lens, index_to_char):
    offset = 0
    all_refs, all_hyps = [], []
    for i, tlen in enumerate(target_lens):
        ref = ''.join([index_to_char[x.item()] for x in targets[offset:offset + tlen]])
        hyp = ''.join([index_to_char[x] for x in preds[i]])
        all_refs.append(ref)
        all_hyps.append(hyp)
        offset += tlen
    return 1.0 - cer(all_refs, all_hyps)

# Greedy decoding for character prediction from model output
def greedy_decode(logits, input_lens, blank_index):
    pred = logits.argmax(dim=2)
    results = []
    for i in range(pred.size(0)):
        tokens, prev = [], None
        for j in range(input_lens[i]):
            idx = pred[i, j].item()
            if idx != blank_index and idx != prev:
                tokens.append(idx)
            prev = idx
        results.append(tokens)
    return results

def train_ctc(model, train_loader, val_loader, optimizer, loss_fn, blank_index, index_to_char,
              epochs=30, patience=5, device='cuda'):

    model.to(device)
    best_val_loss = float('inf')
    patience_counter = 0
    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0
        total_train_acc = 0
        total_batches = 0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1} - Training (CTC)"):
            specs, targets_flat, input_lens, target_lens = batch
            if any(t < 2 for t in target_lens): continue

            specs, targets_flat = specs.to(device), targets_flat.to(device)
            input_lens, target_lens = input_lens.to(device), target_lens.to(device)

            optimizer.zero_grad()
            logits = model(specs)
            log_probs = F.log_softmax(logits, dim=2).transpose(0, 1)
            loss = loss_fn(log_probs, targets_flat, input_lens, target_lens)

            loss.backward()
            optimizer.step()

            preds = greedy_decode(logits, input_lens, blank_index)
            acc = compute_cer_accuracy(preds, targets_flat, target_lens, index_to_char)

            total_train_loss += loss.item()
            total_train_acc += acc
            total_batches += 1

        avg_train_loss = total_train_loss / total_batches
        avg_train_acc = total_train_acc / total_batches
        train_losses.append(avg_train_loss)
        train_accuracies.append(avg_train_acc)

        # Validation
        model.eval()
        total_val_loss = 0
        total_val_acc = 0
        total_val_batches = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1} - Validation (CTC)"):
                specs, targets_flat, input_lens, target_lens = batch
                if any(t < 2 for t in target_lens): continue

                specs, targets_flat = specs.to(device), targets_flat.to(device)
                input_lens, target_lens = input_lens.to(device), target_lens.to(device)

                logits = model(specs)
                log_probs = F.log_softmax(logits, dim=2).transpose(0, 1)
                loss = loss_fn(log_probs, targets_flat, input_lens, target_lens)

                preds = greedy_decode(logits, input_lens, blank_index)
                acc = compute_cer_accuracy(preds, targets_flat, target_lens, index_to_char)

                total_val_loss += loss.item()
                total_val_acc += acc
                total_val_batches += 1

        avg_val_loss = total_val_loss / total_val_batches
        avg_val_acc = total_val_acc / total_val_batches
        val_losses.append(avg_val_loss)
        val_accuracies.append(avg_val_acc)

        print(f"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | CER Acc: {avg_train_acc:.4f} | Val Loss: {avg_val_loss:.4f} | Val CER Acc: {avg_val_acc:.4f}")

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print("⛔ Early stopping (CTC)")
                break

    return train_losses, train_accuracies, val_losses, val_accuracies

def train_rnnt(model, train_loader, val_loader, optimizer, loss_fn, blank_index, index_to_char,
               epochs=30, patience=5, device='cuda'):

    model.to(device)
    best_val_loss = float('inf')
    patience_counter = 0
    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0
        total_train_acc = 0
        total_batches = 0

        for batch in tqdm(train_loader, desc=f"Epoch {epoch+1} - Training (RNN-T)"):
            specs, targets_flat, input_lens, target_lens = batch
            if any(t < 2 for t in target_lens): continue

            specs, targets_flat = specs.to(device), targets_flat.to(device)
            input_lens, target_lens = input_lens.to(device), target_lens.to(device)

            batch_size = specs.size(0)
            target_max_len = target_lens.max().item()
            targets_in = torch.zeros((batch_size, target_max_len), dtype=torch.long, device=device)
            offset = 0
            for i in range(batch_size):
                l = target_lens[i].item()
                if l > 1:
                    targets_in[i, :l-1] = targets_flat[offset:offset+l-1]
                offset += l

            optimizer.zero_grad()
            logits = model(specs, targets_in)  # (B, T_enc, T_dec, V)
            mean_logits = logits.mean(dim=2)
            log_probs = F.log_softmax(mean_logits, dim=2).transpose(0, 1)
            loss = loss_fn(log_probs, targets_flat, input_lens, target_lens)

            loss.backward()
            optimizer.step()

            preds = greedy_decode(mean_logits, input_lens, blank_index)
            acc = compute_cer_accuracy(preds, targets_flat, target_lens, index_to_char)

            total_train_loss += loss.item()
            total_train_acc += acc
            total_batches += 1

        avg_train_loss = total_train_loss / total_batches
        avg_train_acc = total_train_acc / total_batches
        train_losses.append(avg_train_loss)
        train_accuracies.append(avg_train_acc)

        # Validation
        model.eval()
        total_val_loss = 0
        total_val_acc = 0
        total_val_batches = 0

        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"Epoch {epoch+1} - Validation (RNN-T)"):
                specs, targets_flat, input_lens, target_lens = batch
                if any(t < 2 for t in target_lens): continue

                specs, targets_flat = specs.to(device), targets_flat.to(device)
                input_lens, target_lens = input_lens.to(device), target_lens.to(device)

                batch_size = specs.size(0)
                target_max_len = target_lens.max().item()
                targets_in = torch.zeros((batch_size, target_max_len), dtype=torch.long, device=device)
                offset = 0
                for i in range(batch_size):
                    l = target_lens[i].item()
                    if l > 1:
                        targets_in[i, :l-1] = targets_flat[offset:offset+l-1]
                    offset += l

                logits = model(specs, targets_in)
                mean_logits = logits.mean(dim=2)
                log_probs = F.log_softmax(mean_logits, dim=2).transpose(0, 1)
                loss = loss_fn(log_probs, targets_flat, input_lens, target_lens)

                preds = greedy_decode(mean_logits, input_lens, blank_index)
                acc = compute_cer_accuracy(preds, targets_flat, target_lens, index_to_char)

                total_val_loss += loss.item()
                total_val_acc += acc
                total_val_batches += 1

        avg_val_loss = total_val_loss / total_val_batches
        avg_val_acc = total_val_acc / total_val_batches
        val_losses.append(avg_val_loss)
        val_accuracies.append(avg_val_acc)

        print(f"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | CER Acc: {avg_train_acc:.4f} | Val Loss: {avg_val_loss:.4f} | Val CER Acc: {avg_val_acc:.4f}")

        if patience > 0 and avg_val_loss >= best_val_loss:
            patience_counter += 1
            if patience_counter >= patience:
                print("⛔ Early stopping (RNN-T)")
                break
        else:
            best_val_loss = avg_val_loss
            patience_counter = 0

    return train_losses, train_accuracies, val_losses, val_accuracies

def decode_indices(indices, index_to_char):
    return ''.join([index_to_char[i] for i in indices]).replace('|', ' ').strip()

def evaluate_and_plot_model(
    model,
    test_loader,
    loss_fn,
    blank_index,
    index_to_char,
    train_losses,
    val_losses,
    train_accuracies,
    val_accuracies,
    model_name="Model",
    device='cuda'
    ):
    model.eval()
    model.to(device)

    total_loss = 0
    total_acc = 0
    total_batches = 0

    all_refs = []
    all_preds = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc=f"Evaluating {model_name} on Test Set"):
            if batch is None:
                continue
            specs, targets_flat, input_lens, target_lens = batch
            specs, targets_flat = specs.to(device), targets_flat.to(device)
            input_lens, target_lens = input_lens.to(device), target_lens.to(device)

            # Check if the model is RNN-T and provide target input if necessary
            if model_name == "RNN-T":
                batch_size = specs.size(0)
                target_max_len = target_lens.max().item()
                targets_in = torch.zeros((batch_size, target_max_len), dtype=torch.long, device=device)
                offset = 0
                for i in range(batch_size):
                    l = target_lens[i].item()
                    if l > 1:
                        targets_in[i, :l - 1] = targets_flat[offset:offset + l - 1]
                    offset += l
                logits = model(specs, targets_in)  # Pass target input for RNN-T
                # Average the logits over the decoder time steps
                logits = logits.mean(dim=2)
            else:
                logits = model(specs)  # For CTC or other models

            log_probs = F.log_softmax(logits, dim=2).transpose(0, 1)
            loss = loss_fn(log_probs, targets_flat, input_lens, target_lens)

            preds = greedy_decode(logits, input_lens, blank_index)
            acc = compute_cer_accuracy(preds, targets_flat, target_lens, index_to_char)

            total_loss += loss.item()
            total_acc += acc
            total_batches += 1

            offset = 0
            for i, tlen in enumerate(target_lens):
                ref = decode_indices(targets_flat[offset:offset + tlen].tolist(), index_to_char)
                hyp = decode_indices(preds[i], index_to_char)
                all_refs.append(ref)
                all_preds.append(hyp)
                offset += tlen

    avg_loss = total_loss / total_batches
    avg_acc = total_acc / total_batches
    avg_wer = wer(all_refs, all_preds)
    avg_cer = cer(all_refs, all_preds)

    print(f"\n[{model_name} - Test Set]")
    print(f"Loss      : {avg_loss:.4f}")
    print(f"Char Acc  : {avg_acc:.4f}")
    print(f"WER       : {avg_wer:.4f}")
    print(f"CER       : {avg_cer:.4f}")

    # Plotting curves
    plt.figure(figsize=(14, 6))

    # Loss
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Val Loss")
    plt.title(f"{model_name} - Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()

    # Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(train_accuracies, label="Train Acc")
    plt.plot(val_accuracies, label="Val Acc")
    plt.title(f"{model_name} - Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Char Accuracy")
    plt.legend()

    plt.tight_layout()
    plt.show()

    return {
        "loss": avg_loss,
        "char_accuracy": avg_acc,
        "wer": avg_wer,
        "cer": avg_cer
    }

# Load data
data_dir = "/content/drive/MyDrive/TORGO"

# Load dataset
#dataset = TorgoDataset(data_dir, char_to_index)
#torch.save(dataset.items, '/content/drive/MyDrive/torgo_items.pt')
dataset = TorgoDataset(
    root_dir="/content/drive/MyDrive/TORGO",  # still required for structure
    char_to_index=char_to_index,
    use_precomputed=True,
    mel_dir="/content/drive/MyDrive/TORGO_mel_preprocessed"
)
print(f"Dataset length: {len(dataset)}")

# Calculate lengths
total_len = len(dataset)
train_len = int(0.7 * total_len)
val_len = int(0.1 * total_len)
test_len = total_len - train_len - val_len

# Perform split
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_fn, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn, pin_memory=True)

# Train CTC model
model_ctc = CTCModel()
optimizer_ctc = torch.optim.Adam(model_ctc.parameters(), lr=1e-4)
loss_fn_ctc = nn.CTCLoss(blank=char_to_index['|'], zero_infinity=True)

ctc_train_loss, ctc_train_acc, ctc_val_loss, ctc_val_acc = train_ctc(
    model=model_ctc,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optimizer_ctc,
    loss_fn=loss_fn_ctc,
    blank_index=char_to_index['|'],
    index_to_char=index_to_char,
    epochs=30,
    patience=5,
    device='cuda'
)

# Train RNN-T model
model_rnnt = RNNTModel()
optimizer_rnnt = torch.optim.Adam(model_rnnt.parameters(), lr=1e-4)
loss_fn_rnnt = nn.CTCLoss(blank=char_to_index['|'], zero_infinity=True)

rnnt_train_loss, rnnt_train_acc, rnnt_val_loss, rnnt_val_acc = train_rnnt(
    model=model_rnnt,
    train_loader=train_loader,
    val_loader=val_loader,
    optimizer=optimizer_rnnt,
    loss_fn=loss_fn_rnnt,
    index_to_char=index_to_char,
    blank_index=char_to_index['|'],
    epochs=30,
    patience=5,
    device='cuda'
)

ctc_results = evaluate_and_plot_model(
    model=model_ctc,
    test_loader=test_loader,
    loss_fn=loss_fn_ctc,
    blank_index=char_to_index['|'],
    index_to_char=index_to_char,
    train_losses=ctc_train_loss,
    val_losses=ctc_val_loss,
    train_accuracies=ctc_train_acc,
    val_accuracies=ctc_val_acc,
    model_name="CTC",
    device='cuda'
)

rnnt_results = evaluate_and_plot_model(
    model=model_rnnt,
    test_loader=test_loader,
    loss_fn=loss_fn_rnnt,
    blank_index=char_to_index['|'],
    index_to_char=index_to_char,
    train_losses=rnnt_train_loss,
    val_losses=rnnt_val_loss,
    train_accuracies=rnnt_train_acc,
    val_accuracies=rnnt_val_acc,
    model_name="RNN-T",
    device='cuda'
)